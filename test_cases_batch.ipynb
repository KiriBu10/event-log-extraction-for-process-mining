{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.apps import SimpleApp\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.db import get_database_schema_execute_all\n",
    "from utils.helper import save_results\n",
    "import os\n",
    "from keys import OPENAI_KEY\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all dbs and all prompts\n",
    "db_list = [x for x in os.listdir('testDBs') if '.' not in x]\n",
    "prompt_list = [x for x in os.listdir('prompts') if '.txt' in x]\n",
    "\n",
    "print(db_list[0:])\n",
    "print(prompt_list[0:])\n",
    "\n",
    "# LLM model and parameters\n",
    "llm_model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Iterate over all dbs and prompt combinations\n",
    "for db in db_list: # all dbs\n",
    "# for db in db_list[0:3]: # BPI2016\n",
    "# for db in db_list[3:6]: # ERP\n",
    "# for db in db_list[8:9]: # P2P\n",
    "# for db in db_list[9:12]: # UWV\n",
    "\n",
    "    path_to_csv_files = 'testDBs/'+db+'/db/'\n",
    "    path_to_csv_schema_file = 'testDBs/'+db+'/csv_schema.xlsx'\n",
    "    path_to_groud_truth_eventlog = 'testDBs/'+db+'/ground-truth-eventlog.csv'\n",
    "    db_output_dir = 'testDBs/'+db+'/example.db'\n",
    "    result_output_dir = 'testDBs/'+db+'/results/'\n",
    "    db_schema = get_database_schema_execute_all(path_to_csv_files = path_to_csv_files,path_to_csv_schema_file=path_to_csv_schema_file, db_output_dir= db_output_dir)\n",
    "\n",
    "    db_specific_prompt = prompt_list.index('prompt-07-'+db.split('-')[0]+'.txt')\n",
    "\n",
    "    # Iterate over all prompts \n",
    "    for pr in prompt_list[0:7]+prompt_list[db_specific_prompt:db_specific_prompt+1]:\n",
    "        \n",
    "        print('TESTCASE:',db, pr)\n",
    "\n",
    "        file = open('prompts/'+pr, \"r\")\n",
    "        file_txt = file.read()\n",
    "        file.close()\n",
    "\n",
    "        # Baseline prompt which is always part of the tested prompt\n",
    "        prompt=(f\"\"\"Consider the following db schema: {db_schema}\"\"\"+file_txt\n",
    "               +f\"\"\"Write one sql statement that returns an event log from the selected tables with the following columns: case_id, activity_id, timestamp. \"\"\"\n",
    "               +f\"\"\"Use quotes for identifiers.\"\"\"\n",
    "               +f\"\"\"Make sure that all columns of the eventlog are interpreted as varchar values.\"\"\"               \n",
    "               +f\"\"\"Return only the complete SQL query, leave out any other comments in the response. Return the query in plain text without markdown syntax.\"\"\")\n",
    "\n",
    "        AgentState = {\"messages\": [prompt]}\n",
    "        a = SimpleApp(path_to_db = db_output_dir, path_to_groud_truth_eventlog=path_to_groud_truth_eventlog, llm_model = llm_model)\n",
    "        result = a.invoke(AgentState)\n",
    "        save_results(chain_response=result, output_dir=result_output_dir, prompt_file=pr)\n",
    "\n",
    "        # print('\\n EXECUTED SQL STATEMENT: \\n', result['agent_response'])\n",
    "        # print('\\n RESULTING TABLE: \\n', result['sqlexecuter'])\n",
    "        try:\n",
    "            print([result['result']])\n",
    "        except:\n",
    "            print([f\"\"\"Can't calculate Precision, Recall and F1. ERROR in SQL\"\"\"])                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all result figures as pdf\n",
    "from utils.plots import plot_parallel_coordinates\n",
    "\n",
    "db_options = ['P2P', 'ERP', 'UWV', 'BPI2016']\n",
    "metric_options = ['precision', 'recall', 'f1']\n",
    "relaxed_options = ['', 'relaxed_','textual_']\n",
    "\n",
    "path_to_dbs = 'testDBs/'\n",
    "\n",
    "for db in db_options:\n",
    "    for metric in metric_options:\n",
    "        for relaxed in relaxed_options:\n",
    "            plot_parallel_coordinates(path_to_dbs, db, metric, relaxed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results as tables in csv format\n",
    "from utils.plots import save_result_tables\n",
    "\n",
    "db_options = ['RunningExample', 'paperj', 'UWV', 'BPI2016']\n",
    "relaxed_options = ['', 'relaxed_','textual_']\n",
    "\n",
    "path_to_dbs = 'testDBs/'\n",
    "\n",
    "for db in db_options:\n",
    "    for relaxed in relaxed_options:\n",
    "        save_result_tables(path_to_dbs, db, relaxed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
